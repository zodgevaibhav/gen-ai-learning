# Ensamble Learning : Getting the answer of multiple models and combine together
    # It is a machine learning paradigm that combines multiple models to improve performance.
    # It can be used for both classification and regression tasks.
    # Can it combinely use classification and regression models? 
        # Yes, ensemble learning can combine both classification and regression models.
        # For example, you can create an ensemble that includes decision trees (for classification) and linear regression (for regression).
# Ensemble learning methods can be broadly categorized into two types:
# 1. Bagging: It involves training multiple models independently and then combining their predictions.
#    Bagging methods are typically used to reduce variance and improve the stability of the model.
#    Examples of bagging methods include:
#    - Random Forest: An ensemble of decision trees trained on different subsets of the data.
#    - Bootstrap Aggregating (Bagging): A general technique for creating ensembles by training models on bootstrapped samples of the data.
#
# 2. Boosting: It involves training models sequentially, where each model tries to correct the errors of the previous one.
#    Boosting methods are typically used to reduce bias and improve the accuracy of the model.
#    Examples of boosting methods include:
#    - AdaBoost: An ensemble method that combines weak classifiers to create a strong classifier.
#    - Gradient Boosting: An ensemble method that builds models sequentially, optimizing a loss function.
#    - XGBoost: An optimized version of gradient boosting that is faster and more efficient.
#    - LightGBM: A gradient boosting framework that uses tree-based learning algorithms.
#    - CatBoost: A gradient boosting library that handles categorical features automatically.
#
# 3. Stacking: It involves training multiple models and then using their predictions as input to a meta-model.
#    Stacking can be used to combine different types of models, such as decision trees, linear models, and neural networks.
#    The meta-model learns to make predictions based on the outputs of the base models.
#    Stacking can improve the overall performance of the ensemble by leveraging the strengths of different models.
#
# Ensemble learning is widely used in machine learning competitions and real-world applications due to its ability to improve model performance and robustness.
## MySQL DB Connection Utility
import mysql.connector
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate

def get_db_connection():
    """Create and return a MySQL database connection."""
    try:
        connection = mysql.connector.connect(
            host='localhost',
            user='saas',
            password='',
        )
        return connection
    except mysql.connector.Error as err:
        print(f"Error: {err}")
        return None
    

def close_db_connection(connection):
    """Close the MySQL database connection."""
    if connection:
        try:
            connection.close()
        except mysql.connector.Error as err:
            print(f"Error closing connection: {err}")
    else:
        print("No connection to close.")

def execute_query(query, params=None):
    """Execute a SQL query and return the result."""
    connection = get_db_connection()
    if connection is None:
        return None
    
    cursor = connection.cursor(dictionary=True)
    try:
        cursor.execute(query, params)
        result = cursor.fetchall()
        return result
    except mysql.connector.Error as err:
        print(f"Error executing query: {err}")
        return None
    finally:
        cursor.close()
        close_db_connection(connection)


def create_query(question, schema):
    # Create chat template
    template = """
    You are a MySQL expert with access to the `case_service_db` database.

    Generate a SQL query based on the given question and schema.

    Rules:
    - Use only the provided schema.
    - Always join on ID columns, casting them to BINARY.
    - In WHERE clauses:
    - Prefer filtering by ID columns first.
    - Cast all columns to BINARY.
    - Use UPPER() for all comparisons.
    - Use LIKE for partial matches, with wildcards (%) between all parts of the search text.
    - Format LIKE clauses as: LIKE UPPER('%Part1%Part2%')
    - Return only the SQL query — no explanations, no markdown.

    Question: {question}
    Schema: {schema}
    """


    prompt_template = ChatPromptTemplate.from_template(template)
    model = ChatOllama(model="llama3", temperature=0.1)
    result = model.invoke(prompt_template.invoke({"question": question, "schema": schema+" schema"}))
    return result.content


def describe_the_diagnosis(text):
    model = ChatOllama(model="llama3", temperature=0.1)
    result = model.invoke("What do you understand from the text, give me summary in english. Dont apology just give transaction whatever you could : "+text)
    return result.content


schema = """

CREATE TABLE case_service_db.`case_diagnosis_info` (
  `case_diagnosis_id` bigint NOT NULL AUTO_INCREMENT,
  `case_id` varchar(255) DEFAULT NULL,
  `created_at` date DEFAULT NULL,
  `detailed_diagnosis_chikitsa` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `diagnosis_title` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `patient_id` varchar(255) DEFAULT NULL,
  `symptoms_lakshana` longtext CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci,
  `patient_name` varchar(255) DEFAULT NULL,
  `updated_diagnosis_date` date DEFAULT NULL,
  PRIMARY KEY (`case_diagnosis_id`)
) ENGINE=InnoDB AUTO_INCREMENT=267367 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

-- patient_service_db.patient_info definition

CREATE TABLE patient_service_db.`patient_info` (
  `patient_code` bigint NOT NULL AUTO_INCREMENT,
  `patient_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
  `address` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `birth_date` date DEFAULT NULL,
  `date_create` date DEFAULT NULL,
  `date_modified` date DEFAULT NULL,
  `first_examination_date` date DEFAULT NULL,
  `patient_gender` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `patient_mobile` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `patient_name_english` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `patient_name_marathi` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `patient_status` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `RefferedBy` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `day_left` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `follow_up_date` date DEFAULT NULL,
  `last_examination_on` date DEFAULT NULL,
  `recent_case_no` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `e_mailid` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `media_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  PRIMARY KEY (`patient_code`)
) ENGINE=InnoDB AUTO_INCREMENT=57259 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

"""

query = create_query("give me last 5 case diagnosis title for patient name in english like 'Abdul'", schema)
#print ("Generated Query:", query)
result = execute_query(query)
diagnosis=[]
if result is not None:
    diagnosis.append(result)

result_string = str(result)

#print("Diagnosis Titles:", result_string)

summary = describe_the_diagnosis(result_string) 

print("Diagnosis Summary:", summary)# MySQL DB Connection Utility
import mysql.connector
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
import re
from matplotlib import pyplot as plt
import json


def get_db_connection():
    """Create and return a MySQL database connection."""
    try:
        connection = mysql.connector.connect(
            host='localhost',
            user='saas',
            password='',
        )
        return connection
    except mysql.connector.Error as err:
        print(f"Error: {err}")
        return None
    

def close_db_connection(connection):
    """Close the MySQL database connection."""
    if connection:
        try:
            connection.close()
        except mysql.connector.Error as err:
            print(f"Error closing connection: {err}")
    else:
        print("No connection to close.")

def execute_query(query, params=None):
    """Execute a SQL query and return the result."""
    connection = get_db_connection()
    if connection is None:
        return None
    
    cursor = connection.cursor(dictionary=True)
    try:
        cursor.execute(query, params)
        result = cursor.fetchall()
        return result
    except mysql.connector.Error as err:
        print(f"Error executing query: {err}")
        return None
    finally:
        cursor.close()
        close_db_connection(connection)


def create_query(question, schema):
    # Create chat template
    template = """
    You are a MySQL expert with access to the database.

    Generate a SQL query based on the given question and schema.

    Rules:
    - Use only the provided schema/databse in query.
    - if required Join on ID columns, casting them to BINARY.
    - use schema name as prefix for all table names.
    - In WHERE clauses:
    - Prefer filtering by ID columns first.
    - Cast all columns to BINARY.
    - Use UPPER() for all comparisons.
    - Use LIKE for partial matches, with wildcards (%) between all parts of the search text.
    - Format LIKE clauses as: LIKE UPPER('%Part1%Part2%')
    - Do not generate any markdown.
    - Return only the SQL query — no explanations
    - Use inner queries as much as possible.
    Question: {question}
    Schema: {schema}
    """

    prompt_template = ChatPromptTemplate.from_template(template)
    model = ChatOllama(model="llama3", temperature=0.1)
    result = model.invoke(prompt_template.invoke({"question": question, "schema": schema+" schema"}))
    return result.content

def clean_sql_output(output):
    # Removes ```sql ... ``` or ```...``` wrappers
    return re.sub(r"^```(?:sql)?\s*|```$", "", output.strip(), flags=re.MULTILINE)


def understand_cateforize_context(question):
    template = """
    You are a Prompt Engineering expert.

    You need to understand the context of the given text and categorize it into specified categories & generate Prompt for the category to use in AI Model.

    Below are the categories:
          - SQLQueryGeneration
          - DataAnalysis
          - DataVisualization
    Rules:
        Create a JSON Object with the following structure, so that I can use it my code
                {{  
                    "SQLQueryGeneration": "Generate SQL Query......"                
                    "DataAnalysis":"DataAnalysis explanation......"                
                    "DataVisualization":"DataVisualization explanation......"                
                }}
        For any data required to generate SQL query, you can consider SQLQueryGeneration.
        Do not use any other format, just return JSON Object.
        Do not give any explanation, just return JSON Object.
        If question/prompt have reference to generate visualization, then add "DataVisualization" category with explanation to generate visualization.
        If question/prompt have reference to generate analysis, then add "DataAnalysis" category with explanation to generate analysis.
        If question/prompt have reference to generate SQL query, then add "SQLQueryGeneration" category with explanation to generate SQL query.
        Any required data must need to fetch from the database, so you can use SQL query to fetch data.
        You just generate proqmpt for the category, do not generate any SQL query.
    Question: {question}
    """


    prompt_template = ChatPromptTemplate.from_template(template)
    model = ChatOllama(model="llama3", temperature=0.1)
    result = model.invoke(prompt_template.invoke({"question": question}))
    return result.content

def generate_matplot_lib_compatible_data(text):
    template = """
    You are a Matplotlib expert.

    Rules:
        - Understand what kind of plot is needed based on the text.
        - Generate JSON object with the following structure:
                {{
                    "plot_type": "pie",  # or "bar", "line", etc.
                    "data": {{
                        "labels": ["label1", "label2", ...],
                        "values": [value1, value2, ...]
                    }},
                    "title": "Title of the plot"
                }}
        - Do not generate any SQL query.
        - Do not give any explanation, just return JSON Object.
        - Do not generate any markdown.
        - Do not generate any code.
        - Generate only JSON object with the required data for the plot.

    Text: {text}
    """

    prompt_template = ChatPromptTemplate.from_template(template)
    model = ChatOllama(model="llama3", temperature=0.1)
    result = model.invoke(prompt_template.invoke({"text": text}))
    return result.content


def main():

    schema = """
    -- patient_service_db.patient_info definition

    CREATE TABLE patient_service_db.`patient_info` (
    `patient_code` bigint NOT NULL AUTO_INCREMENT,
    `patient_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci NOT NULL,
    `address` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `birth_date` date DEFAULT NULL,
    `date_create` date DEFAULT NULL,
    `date_modified` date DEFAULT NULL,
    `first_examination_date` date DEFAULT NULL,
    `patient_gender` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `patient_mobile` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `patient_name_english` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `patient_name_marathi` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `patient_status` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `RefferedBy` varchar(100) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `day_left` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `follow_up_date` date DEFAULT NULL,
    `last_examination_on` date DEFAULT NULL,
    `recent_case_no` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `e_mailid` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    `media_id` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci DEFAULT NULL,
    PRIMARY KEY (`patient_code`)
    ) ENGINE=InnoDB AUTO_INCREMENT=57259 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

    """

    prompt = "Can you fetch data categorise patients in infants, kids, middle agae, old age with count according to date of birth. After generate data create pie chart for the same. Also give me summary of the data in english."
    #prompt = "Give me patient cound by geneder and create bar chart for the same."

    context_str = understand_cateforize_context(prompt)
    print("Context Understanding:", context_str)
    try:
        context = json.loads(context_str)
    except Exception as e:
        print("Error parsing context JSON:", e)
        context = {}

    query = context.get("SQLQueryGeneration", None)
    dataAnalysis = context.get("DataAnalysis", None)
    dataVisualizaion =  context.get("DataVisualization",None)


    query_result = None
    if(query is not None):
        query = create_query(prompt, schema)
        print ("Generated Query:", query)
        query_result = execute_query(clean_sql_output(query))
        print ("Query Result : ", query_result)

    if query_result is None or len(query_result) == 0: exit(0)

    if(dataAnalysis is not None):
        model = ChatOllama(model="llama3", temperature=0.1)
        result = model.invoke("I have data : "+str(query_result)+" and I need to analyse it as per the following instruction : "+dataAnalysis)
        print("Data Analysis Result:", result.content)

    dataVisualizaion=""
    if(dataVisualizaion is not None):
        result = generate_matplot_lib_compatible_data(query_result)
        result = json.loads(result)

        print("Data Visualization Result:", result)
        plt.title(result.get("title", "Generated Plot"))
        if result.get("plot_type") == "bar":
            plt.bar(result.get("data", {}).get("labels", []), result.get("data", {}).get("values", []))
        elif result.get("plot_type") == "line":
            plt.plot(result.get("data", {}).get("labels", []), result.get("data", {}).get("values", []))
        elif result.get("plot_type") == "pie":
            plt.pie(result.get("data", {}).get("values", []), labels=result.get("data", {}).get("labels", []))
        plt.show()




main()# pip install transformers
# pip install sentencepiece
# pip install sacremoses


from transformers import MarianMTModel, MarianTokenizer

model_name = 'Helsinki-NLP/opus-mt-mr-en'
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

text = "तुझ नाव काय आहे?"
tokens = tokenizer(text, return_tensors="pt")
translation = model.generate(**tokens)
print(tokenizer.decode(translation[0], skip_special_tokens=True))
import streamlit as st
from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate






import tempfile

def create_embeddings_from_pdf(pdf_file):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    docs = []
    # Save the uploaded file to a temporary location
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(pdf_file.read())
        tmp_file_path = tmp_file.name
    # Load the PDF file using PyPDFLoader
    loader = PyPDFLoader(tmp_file_path)
    # Load the pages from the PDF file
    pages = loader.load_and_split()
    # Create Document objects for each page
    id=1
    for page in pages:
        docs.append(Document(id=id, page_content=page.page_content))
        id=id+1

    # Split the documents into smaller chunks for better embedding and retrieval performance.
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
          # Chunk size is the maximum length of each chunk, and chunk overlap is the number of characters that overlap between chunks.
          # Overlping meaning, the last 200 characters of the previous chunk will be included in the next chunk.
          # Overlapping helps to maintain context between chunks.
    docs = splitter.split_documents(docs)

    vector_store = Chroma(embedding_function=OllamaEmbeddings(model="llama3"), collection_name="vaibhav_zodge", persist_directory="./chroma_db")
    vector_store.add_documents(docs)
    
    # Store vector store to session state for later use
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = vector_store
    

def rag_pipeline(context, question="Who is Vaibhav Zodge?"):    
    template = """
    You are a helpful assistant. Answer the question based on the context provided.
    Answer with not more than 100 words.
    Don't talk about context, just answer the question.
    If you don't know the answer, say "I don't know".
    Question: {question}
    Context: {context}
    Answer:
    """

    prompt_template = ChatPromptTemplate.from_template(template)
    prompt = prompt_template.invoke({"question":question, "context":context})
    #print("\n\n********* Prompt : "+str(prompt))

    model = ChatOllama(model="llama3", temperature=0.1)

    response = model.invoke(prompt)
    return response.content


################# MAIN FUNCTION ####################

st.title("RAG Pipeline Chatbot")
st.header("Upload PDF and Chat with it")

with st.sidebar:
    st.subheader("Upload PDF")
    uploaded_file = st.file_uploader("Choose a PDF file", type="pdf")
if uploaded_file is not None:
    # Create embeddings from the uploaded PDF file
    create_embeddings_from_pdf(uploaded_file)
    st.success("PDF uploaded and embeddings created successfully!")

# Check if the vector store is already created
if "vector_store" in st.session_state:
    st.success("Vector store is ready to use!")

question = st.chat_input("Ask a question about the PDF content:")
if question:
    # Search for similar vectors in the vector store
    context = st.session_state.vector_store.search(question, search_type="similarity", k=5)
    
    # Generate response using RAG pipeline
    response = rag_pipeline(context, question)
    
    # Display the response
    st.write(response)



# Build knowledge base using Langchain and Ollama


from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter



def load_pdf_return_documents(file_path="./VaibhavZodge.pdf"):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    docs = []
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    id=1
    for document in documents:
        docs.append(Document(id=id, page_content=document.page_content))
        id=id+1

    return docs


# Create an instance of the OllamaEmbeddings class
obj_embeddings = OllamaEmbeddings(model = "llama3")

# Create an instance of the Chroma class to store the embeddings
# When we give embedding object to the vector store, it initialize with the given embedding function.
    # Ref: https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html#langchain_chroma.vectorstores.Chroma
vector_store = Chroma(embedding_function=obj_embeddings, collection_name="vaibhav_zodge", persist_directory="./chroma_db")

# Load the PDF documents and split them into chunks
# The text splitter is used to split the documents into smaller chunks for better embedding
# and retrieval performance.
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

# Load the PDF documents and split them into chunks
docs = splitter.split_documents(load_pdf_return_documents())

# Add the documents to the vector store
vector_store.add_documents(docs)

# Search for similar vectors in the vector store
similar_vectors = vector_store.search("Does vaibhav have github ?", search_type="similarity", k=5)

print("\n\n********* Similar Vectors : "+str(similar_vectors))



from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader
from langchain_ollama import OllamaEmbeddings
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate


def load_pdf_return_documents(file_path="./VaibhavZodge.pdf"):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    docs = []
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    id=1
    for document in documents:
        docs.append(Document(id=id, page_content=document.page_content))
        id=id+1

    return docs

def rag_pipeline(context, question="Who is Vaibhav Zodge?"):    
    template = """
    You are a helpful assistant. Answer the question based on the context provided.
    Answer with not more than 100 words.
    Don't talk about context, just answer the question.
    If you don't know the answer, say "I don't know".
    Question: {question}
    Context: {context}
    Answer:
    """

    prompt_template = ChatPromptTemplate.from_template(template)
    prompt = prompt_template.invoke({"question":question, "context":context})
    #print("\n\n********* Prompt : "+str(prompt))

    model = ChatOllama(model="llama3", temperature=0.1)

    response = model.invoke(prompt)
    return response.content


# Create an instance of the OllamaEmbeddings class
obj_embeddings = OllamaEmbeddings(model = "llama3")

# Create an instance of the Chroma class to store the embeddings
# When we give embedding object to the vector store, it initialize with the given embedding function.
    # Ref: https://python.langchain.com/api_reference/chroma/vectorstores/langchain_chroma.vectorstores.Chroma.html#langchain_chroma.vectorstores.Chroma
vector_store = Chroma(embedding_function=obj_embeddings, collection_name="vaibhav_zodge", persist_directory="./chroma_db")

# Load the PDF documents and split them into chunks
# The text splitter is used to split the documents into smaller chunks for better embedding
# and retrieval performance.
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

# Load the PDF documents and split them into chunks
docs = splitter.split_documents(load_pdf_return_documents())

# Add the documents to the vector store

vector_store.add_documents(docs)

# Search for similar vectors in the vector store
question = "Explain Vaibhav Zodge in point by point format"
context = vector_store.search(question, search_type="similarity", k=5)

response = rag_pipeline(context, question)

print(response)


from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader
from langchain_ollama import OllamaEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.documents import Document



def load_pdf_return_documents(file_path="./VaibhavZodge.pdf"):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    docs = []
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    id=1
    for document in documents:
        docs.append(Document(id=id, page_content=document.page_content))
        id=id+1

    return docs


# Create an instance of the OllamaEmbeddings class
obj_embeddings = OllamaEmbeddings(model = "llama3")

# Create an instance of the InMemoryVectorStore class to store the embeddings
# When we give embedding object to the vector store, it initialize with the given embedding function.
    # Ref: https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.in_memory.InMemoryVectorStore.html#langchain_core.vectorstores.in_memory.InMemoryVectorStore.dump
vector_store = InMemoryVectorStore(embedding=obj_embeddings)

vector_store.add_documents(load_pdf_return_documents())

similar_vectors = vector_store.search("Does vaibhav have github ?", search_type="similarity", k=1)

print("\n\n********* Similar Vectors : "+str(similar_vectors))

# Save the vector store to a file
##vector_store.dump("vector_store.json")
from langchain_ollama import ChatOllama

model = ChatOllama(
    model="llama3",
    max_tokens=100,
)

prompt = "What is Vaibhav Zodge ?"

response = model.invoke(prompt)

print(response.content)from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader
from langchain_ollama import OllamaEmbeddings

def creating_embedding():
    embeddings = OllamaEmbeddings(model = "llama3")
    embedded_data = embeddings.embed_documents(str(load_pdf()))
    return embedded_data

def load_pdf(file_path="./VaibhavZodge.pdf"):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    loader = PyPDFLoader(file_path)
    documents = loader.load()
    return documents

print(creating_embedding());
from langchain_ollama import ChatOllama
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, PyPDFDirectoryLoader
from langchain_core.documents import Document


def load_pdf(file_path="./VaibhavZodge.pdf"):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    loader = PyPDFLoader(file_path)
    documents = loader.load() # It returns a list of Document objects, each containing the text content of the PDF page.
    return documents

def load_pdf_return_documents(file_path="./VaibhavZodge.pdf"):
    """Load a PDF file and return its content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html
    loader = PyPDFLoader(file_path)
    documents = loader.load() # It returns a list of Document objects, each containing the text content of the PDF page.
    docs = []
    id=1
    for document in documents:
        docs.append(Document(id=id, page_content=document.page_content))
        id=id+1
    return docs

def load_from_web(url="https://github.com/zodgevaibhav"):
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html
    loader = WebBaseLoader(url)
    documents = loader.load()
    return documents


def load_pdf_directory(directory_path="./"):
    """Load all PDF files from a directory and return their content."""
    # https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFDirectoryLoader.html
    loader = PyPDFDirectoryLoader(directory_path)
    documents = loader.load()
    return documents

documents = load_pdf_return_documents()
print("\n\n********* Documents : "+str(documents))

# documents = load_from_web()
# print("\n\n ********* Web : "+str(documents))




# GenAI meaning Generated AI
    # It can generate Textual data, video, audio
    # All of above data is un structured data
    # To deal with unstructured data we need to use NN (Neural Network)

    
# Install Olama : Olama is a free and open-source platform for running large language models (LLMs) on your local machine.
# Download llama3 using command line 
    # olama pull llama3
    # olama run llama3

# Install python packages
# pip3 install langchain langchain-ollama langchain-openai langchain-community pypdf langchain-community 
# pip install -qU langchain-chromafrom langchain_ollama import OllamaLLM

llm = OllamaLLM(
    model="llama3",
    max_tokens=100
)
prompt = "What is the capital of France?"
# Call the Ollama API
response = llm.invoke(prompt)
print(response)# Import required libraries
import os 
from langchain_openai import ChatOpenAI

# Create object of ChatOpenAI to access OpenAI API
llm = ChatOpenAI(
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    model="gpt-4o",
    max_tokens=6,
)

prompt = "What is the capital of France?"

# Call the OpenAI API
response = llm.invoke(prompt)
print(response.content)
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage


llm = ChatOllama(
    model="llama3",
    max_tokens=100,
    temperature=0.7, # Temperature is a parameter that controls the randomness of the model's output. A higher temperature (e.g., 1.0) makes the output more random, while a lower temperature (e.g., 0.2) makes it more focused and deterministic.
    # top_p=0.9, # Top-p sampling is another way to control the randomness of the model's output. It considers only the most probable tokens whose cumulative probability exceeds the top_p value.
    # top_k=50, # Top-k sampling is a method that limits the model to consider only the top k most probable tokens at each step.
    # frequency_penalty=0.5, # Frequency penalty is a parameter that reduces the likelihood of the model repeating the same token multiple times in the output.
    # presence_penalty=0.5, # Presence penalty is a parameter that increases the likelihood of the model introducing new tokens in the output.
    # stop=["\n"] # Stop sequence is a string that tells the model when to stop generating text. It can be used to control the length of the output.
     n=2 # Number of responses to generate. If set to 1, the model will generate a single response. If set to a higher number, the model will generate multiple responses.
)

# Prompts : is a string that contains the prompt to be sent to the LLM
# Types of Prompts:
# 1. System Prompt: A system prompt is a message that sets the behavior of the model. It can be used to instruct the model on how to respond to user inputs.
# 2. User Prompt: A user prompt is a message that is sent to the model by the user. It can be used to ask questions or provide information to the model.
# 3. AI Prompt: An AI prompt is a message that is generated by the model in response to a user prompt. It can be used to provide information or answer questions.

messages = []

while True:
    prompt = input("User : ")
    if prompt.lower() == "exit":
        break

    messages.append(HumanMessage(prompt)) # The user message is added to the history so that the model can use it to generate a more relevant response to the next user input
    response = llm.invoke(messages)
    messages.append(AIMessage(response.content)) # The AI message is added to the history so that the model can use it to generate a more relevant response to the next user input.
    
    print("Bot : "+response)

print("Goodbye!")

import streamlit as st
import pandas as pd
import numpy as np
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage


llm = ChatOllama(
    model="llama3",
    max_tokens=100,
)

# Maintain the history of the conversation
messages = []
st.title("Chatbot with History")
st.write("Welcome to my chatbot")

prompt = st.chat_input("Enter your message:")

if prompt:
    messages.append(HumanMessage(prompt)) # The user message is added to the history so that the model can use it to generate a more relevant response to the next user input
    response = llm.invoke(messages)
    messages.append(AIMessage(response.content)) # The AI message is added to the history so that the model can use it to generate a more relevant response to the next user input.
    st.write("Bot : "+response.content)
from flask import Flask, request, jsonify, render_template
from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage


llm = ChatOllama(
    model="llama3",
    max_tokens=100
)

messages = []

app = Flask(__name__)

def root():
    return render_template('index.html')

@app.route('/', methods=['GET'])
def index():
    return root()


@app.route('/api/chat', methods=['POST'])
def chat():
    data = request.get_json()
    user_message = data.get('message')
    
    # Here you would typically call your LLM model to get a response
    # For demonstration, we'll just echo the user message
    print("******** User message : "+user_message)
    return jsonify({'response': getChatResponse(user_message)})



def getChatResponse(prompt):
    messages.append(HumanMessage(prompt)) # The user message is added to the history so that the model can use it to generate a more relevant response to the next user input
    response = llm.invoke(messages)
    messages.append(AIMessage(response.content)) # The AI message is added to the history so that the model can use it to generate a more relevant response to the next user input.
    return response.content



app.run(host="0.0.0.0", port=5400, debug=True)


from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage


llm = ChatOllama(
    model="llama3",
    max_tokens=100
)

# Promopt : is a string that contains the prompt to be sent to the LLM
# Types of Prompts:
# 1. System Prompt: A system prompt is a message that sets the behavior of the model. It can be used to instruct the model on how to respond to user inputs.
# 2. User Prompt: A user prompt is a message that is sent to the model by the user. It can be used to ask questions or provide information to the model.
# 3. AI Prompt: An AI prompt is a message that is generated by the model in response to a user prompt. It can be used to provide information or answer questions.

while True:
    prompt = input("User : ")
    if prompt.lower() == "exit":
        break

    response = llm.invoke(prompt)
    print("Bot : "+response.content)

print("Goodbye!")

from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

llm = ChatOllama(
    model="llama3",
    max_tokens=100
)
# Promopt : is a string that contains the prompt to be sent to the LLM
# Types of Prompts:
# 1. System Prompt: A system prompt is a message that sets the behavior of the model. It can be used to instruct the model on how to respond to user inputs.    
# 2. User Prompt: A user prompt is a message that is sent to the model by the user. It can be used to ask questions or provide information to the model.
# 3. AI Prompt: An AI prompt is a message that is generated by the model in response to a user prompt. It can be used to provide information or answer questions.

context = SystemMessage(
    content="You are a Java developer."
)
human_message = HumanMessage("Tell me a joke")
response = llm.invoke([context, human_message])
print(response.content)

print("--------------------------------------------------")

context = SystemMessage(
    content="You are a Python developer."
)
human_message = HumanMessage("Tell me a joke")
response = llm.invoke([context, human_message])
print(response.content)import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

df = pd.read_csv("hearing_test.csv")

# Exploratory Data Analysis (EDA)
# print("############# Columns : ")
# print(df.columns)
# print("############# Head : ")
# print(df.head())
# print("############# Tail : ")
# print(df.tail())
# print("############# Info : ")
# print(df.info())  # No need for print()

# print("############# Describe : ")
# print(df.describe())

print("############# Correlation : ")
print(df.corr())

x = df.drop(columns=['test_result'], axis=1)
y = df['test_result']

# Split the data into training and testing sets
# 80% of the data will be used for training and 20% for testing
# random_state is used to ensure that the data is split in the same way every time
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=23123123)

# Build the model
model = LogisticRegressionCV()

model.fit(x_train, y_train)  # Train the model using the training sets

# Test the model
# Predict the test set results

y_pred = model.predict(x_test)

# Get the confustion matrix
# Confusion matrix is a table that is often used to describe the performance of a classification model
# It is a summary of the prediction results on a classification problem
# It shows the ways in which your classification model is confused when it makes predictions
# Confusion Matrix:
#    [[359  46]
#    [ 40 555]]
#    True Positive: 359
#    True Negative: 555
#    False Positive: 46
#    False Negative: 40
# This means, the model correctly predicted 359 positive cases and 555 negative cases.
# It incorrectly predicted 46 positive cases as negative and 40 negative cases as positive.

confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(confusion)

# Get the accuricy score
# Accuracy Score:
#    0.914
# Accuracy is the ratio of correctly predicted instances to the total instances
# It is a measure of how often the classifier is correct
# Accuracy = (TP + TN) / (TP + TN + FP + FN)
# where TP = True Positive, TN = True Negative, FP = False Positive, FN = False Negative
# (359 + 555) / (359 + 555 + 46 + 40) = 0.914

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy Score:")
print(accuracy)# Author: Vaibhav Zodge
# Outcome: Learn Data Cleansing 
#          Model Evaluation using Confusion Matrix, Precision, F1 Score

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, classification_report

################### Data Load and Primary Analysis##################

df = pd.read_csv("iris.csv") 
# # Read first few records
# print("############# Head : ")
# print(df.head())
# # Read last few records
# print("############# Tail : ")
# print(df.tail())


# # Get some basic information about the dataset
# print("############# Info : ")
# print(df.info())  # No need for print()

############################ Data Cleansing ########################

# On basic info will see species is textual and not numeric
# So we need to convert the species into numeric
# Label encoding is a technique used to convert categorical variables into numerical values
# It assigns a unique integer to each category in the variable
# For example, if we have a variable with three categories: 'red', 'green', and 'blue',
# label encoding would assign the values 0, 1, and 2 to these categories respectively
encoder = LabelEncoder()
encoder.fit(df['species']) # Fit the encoder to the species column 
                           # This will create a mapping of the unique values in the species column to integers
print(encoder.classes_ )

df['species'] = encoder.transform(df['species']) # Transform the species column
                                                 # This will replace the original values in the species column with their corresponding integer values
##   OR use below method which fit and transform in one step
# df['species'] = encoder.fit_transform(df['species']) # Transform the species column

print("############# Info : ")
df.info()  # No need for print()

############################ Data Visulization/Analysis ########################

#plt.scatter(df['sepal_length'], df['species'])
#plt.show()
# We will observe data visualization is not helpful because data is categorical data and not linear data
# Categorical data is not linear data, meaning it does not follow a straight line. 
# Categorical data is often represented as discrete values or categories, rather than continuous values.

print(df.corr())

#              sepal_length  sepal_width  petal_length  petal_width   species
#sepal_length      1.000000    -0.109369      0.871754     0.817954  0.782561
#sepal_width      -0.109369     1.000000     -0.420516    -0.356544 -0.419446
#petal_length      0.871754    -0.420516      1.000000     0.962757  0.949043
#petal_width       0.817954    -0.356544      0.962757     1.000000  0.956464
#species           0.782561    -0.419446      0.949043     0.956464  1.000000
# The correlation matrix shows the correlation coefficients between different features in the dataset.
# The values range from -1 to 1, where: 1 indicates a perfect positive correlation, -1 indicates a perfect negative correlation,

# Analysis of correlation:
    # Strong correlations:
    # - petal_length and petal_width (0.962757)
    # - petal_length and species (0.949043)
    # - petal_width and species (0.956464)
    # - sepal_length and petal_length (0.871754)
    # - sepal_length and petal_width (0.817954)
    # - sepal_length and species (0.782561)

    # Weak correlations:
    # - sepal_length and sepal_width (-0.109369)
    # - sepal_width and petal_length (-0.420516)
    # - sepal_width and petal_width (-0.356544)
    # - sepal_width and species (-0.419446)

############################ Split the data ########################
x = df.drop('species', axis=1)  # Features: all columns except 'species'
y = df['species']  # Target: 'species' column

# Split the data into training and testing sets
# 80% of the data will be used for training and 20% for testing
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2131546)

############################ Create Model ########################
model = LogisticRegression()
model.fit(x_train, y_train) # Train the model using the training sets
# Test the model
y_pred = model.predict(x_test)
print("Predicted Species: ", y_pred)
print("Actual Species: ", y_test.values)


############################ Model Evaluations/Metrics ########################

print("############# Accuracy : ")
print(accuracy_score(y_test, y_pred)) # Accuracy of the model
print("############# Confusion Matrix : ")
print(confusion_matrix(y_test, y_pred)) # Confusion matrix of the model
# The confusion matrix is a table that is often used to describe the performance of a classification model

# Precision is the ratio of true positive predictions to the total number of positive predictions made by the model
# Which means how many of the predicted positive cases were actually positive
# Precision = TP / (TP + FP), where TP is true positives and FP is false positives
# average : {'micro', 'macro', 'samples', 'weighted'}
# micro : Calculate metrics globally by counting the total true positives, false negatives and false positives.
# macro : Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.
# samples : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification).
# weighted : Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).
precision = precision_score(y_test, y_pred, average='weighted')
print("############# Precision : ") 
print(precision) # Precision of the model

# F1 score is the harmonic mean of precision and recall
# F1 Score = 2 * (Precision * Recall) / (Precision + Recall)
# F1 Score is a measure of a model's accuracy on a dataset
# It is the weighted average of precision and recall
# F1 Score is a good measure of a model's performance when the class distribution is imbalanced
# F1 Score is a better measure than accuracy for imbalanced datasets
f1Score = f1_score(y_test, y_pred, average='weighted')
print("############# F1 Score : ")
print(f1Score) # F1 Score of the model


print("############# Classification Report : ")
print(classification_report(y_test, y_pred)) # Classification report of the model
# The classification report is a summary of the precision, recall, F1 score, and support for each class in the dataset
# The classification report is a useful tool for evaluating the performance of a classification model
# It provides a detailed breakdown of the model's performance for each class in the dataset
# The classification report includes the following metrics:
# - precision: The ratio of true positive predictions to the total number of positive predictions made by the model
# - recall: The ratio of true positive predictions to the total number of actual positive cases in the dataset
# - f1-score: The harmonic mean of precision and recall
# - support: The number of actual occurrences of the class in the specified dataset

############################ Result Visualiation ########################

# identify the records for setosa (0)
plt.scatter(x_test['sepal_length'][y_pred==0], x_test['sepal_width'][y_pred==0], color='red', label='Setosa')

# identify the records for versicolor (1)
plt.scatter(x_test['sepal_length'][y_pred == 1], x_test['sepal_width'][y_pred == 1], color="green", label="versicolor")

# identify the records for virginica (2)
plt.scatter(x_test['sepal_length'][y_pred == 2], x_test['sepal_width'][y_pred == 2], color="blue", label="virginica")

plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('Iris Species Classification')
plt.legend()
plt.show()import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

df = pd.read_csv("car_price_data.csv")

df.info()

print(df['Mileage'].corr(df['Price']))
print(df['Age'].corr(df['Price']))

plt.scatter(df['Price'], df['Age'])
plt.legend(['Age vs Price'])
plt.xlabel("Price")
plt.ylabel("Age")
plt.show()

plt.scatter(df['Price'], df['Mileage'])
plt.legend(['Mileage vs Price'])
plt.xlabel("Price")
plt.ylabel("Mileage")
plt.show()

# Separate the features (independent variables) and the target (dependent variable)
x = df.drop('Price', axis=1)  # Features: all columns except 'Price'
y = df['Price']  # Target: 'Price' column

model = LinearRegression()
model.fit(x, y)

predictions = model.predict(pd.DataFrame([[2,20000]],columns=['Age','Mileage']))
print(predictions)import pandas as pd
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pickle


df = pd.read_csv("car_price_data.csv")

#df.info()
print(df.columns)

print(df['Mileage'].corr(df['Price']))
print(df['Age'].corr(df['Price']))

# Data Scaling Example
# Scaling means transforming data to a specific range
# Two common techniques are Normalization and Standardization   
# MinMaxScaler (Normalization) : Scales data to a fixed range - usually 0 to 1
# StandardScaler (Standardization) : Scales data to have mean=0 and variance=1
## Which means it centers the data around 0 and scales it based on standard deviation
## Example : If mean=50 and std=10, then value 60 will be transformed to (60-50)/10 = 1

scaler = StandardScaler()
df[['Age','Mileage']] = scaler.fit_transform(df[['Age','Mileage']]) # Scaling only feature columns
print("Scaled Data:\n", df.head(), "\n")

plt.scatter( df['Price'],df['Age'])
plt.scatter(df['Price'],df['Mileage'])
plt.xlabel("Price")
plt.ylabel("Age/Mileage")
plt.show()

import pandas as pd
import matplotlib.pyplot as ply
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

df = pd.read_csv('data.csv')

df.info()

corr = df['Speed'].corr(df['BrakingDistance'])
print(corr)

x = df.drop(columns=['BrakingDistance'], axis=1)
y=df['BrakingDistance']

# print(x)

poly = PolynomialFeatures(degree=2)
x_square = poly.fit_transform(x) 
# print(x_square)

model = LinearRegression()
model.fit(x_square, y)

output = model.predict(poly.fit_transform([[120]]))

print("Braking distance at speed 120 is : " )
print(poly.fit_transform([[120]]))

ply.plot(df['Speed'], df['BrakingDistance'])
ply.xlabel('Speed')
ply.ylabel('BrakingDistance')
ply.title('Speed vs BrakingDistance')
ply.show()

# 617*617 = 380689import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Training Data
df = pd.read_csv('data.csv')

X = df.drop(columns=['BrakingDistance'],axis = 1)
y = df['BrakingDistance']

# Linear Regression (straight line)
lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_pred_linear = lin_reg.predict(X)

# Polynomial Regression (degree=2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
poly_reg = LinearRegression()
poly_reg.fit(X_poly, y)
y_pred_poly = poly_reg.predict(X_poly)

# Plot the prediction by Linear and Polynomial Regression
plt.scatter(X, y, color="blue", label="Data points")
plt.plot(X, y_pred_linear, color="red", label="Linear Regression (straight line)")
plt.plot(X, y_pred_poly, color="green", label="Polynomial Regression (curve)")
plt.legend()
plt.xlabel("X")
plt.ylabel("y")
plt.title("Linear vs Polynomial Regression")
plt.show()
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load data
data = pd.read_csv("CarSaleData.csv")
print("Original Data:\n", data.head(), "\n")

# Features (X) and Target (y)
X = data.drop("Price", axis=1)
y = data["Price"]

# -----------------------------
# 1. Label Encoding (for ordinal-like or quick encoding)
# -----------------------------
label_encoder = LabelEncoder()
X_label_encoded = X.copy()
X_label_encoded["Brand"] = label_encoder.fit_transform(X["Brand"])
X_label_encoded["Color"] = label_encoder.fit_transform(X["Color"])
print("Label Encoded Data:\n", X_label_encoded.head(), "\n")

# -----------------------------
# 2. One-Hot Encoding (better for categorical data)
# -----------------------------
column_transformer = ColumnTransformer(
    transformers=[
        ("onehot", OneHotEncoder(sparse_output=False, drop="first"), ["Brand", "Color"])
    ],
    remainder="passthrough"  # keep other columns
)

X_onehot = column_transformer.fit_transform(X)
X_onehot = pd.DataFrame(X_onehot, columns=column_transformer.get_feature_names_out())
print("One-Hot Encoded Data:\n", X_onehot.head(), "\n")

# -----------------------------
# 3. Normalization (Min-Max scaling: values between 0-1)
# -----------------------------
scaler_minmax = MinMaxScaler()
X_normalized = scaler_minmax.fit_transform(X_onehot)
print("Normalized Data (0-1 range):\n", pd.DataFrame(X_normalized).head(), "\n")

# -----------------------------
# 4. Standardization (mean=0, std=1)
# -----------------------------
scaler_standard = StandardScaler()
X_standardized = scaler_standard.fit_transform(X_onehot)
print("Standardized Data (mean=0, std=1):\n", pd.DataFrame(X_standardized).head(), "\n")

# -----------------------------
# 5. Linear Regression Model
# -----------------------------
X_train, X_test, y_train, y_test = train_test_split(X_standardized, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)
print("Train R^2 Score:", model.score(X_train, y_train))
print("Test R^2 Score:", model.score(X_test, y_test))

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

data = pd.read_csv("SpendingData.csv")
X = data.drop("Spendings", axis=1)
y = data["Spendings"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print("X_train:\n", X_train.head(), "\n")
print("X_test:\n", X_test.head(), "\n")
print("y_train:\n", y_train.head(), "\n")
print("y_test:\n", y_test.head(), "\n")import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Data
age = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)
car_value = np.array([9.0, 8.5, 7.0, 5.0, 3.5, 2.0])

# Polynomial Features
poly = PolynomialFeatures(degree=2)  # you can try degree=3 for sharper curve
age_poly = poly.fit_transform(age)

# Fit Polynomial Regression
model = LinearRegression()
model.fit(age_poly, car_value)


age_range_poly = poly.transform(age)
car_pred = model.predict( poly.transform(age))

# Plotting
plt.figure(figsize=(8,5))
plt.plot(age, car_value, '-', label='Original Data', color='red')
plt.plot(age, car_pred, '-', label='Polynomial Fit', color='blue')
plt.title("Car Value Depreciation with Age")
plt.xlabel("Age (Years)")
plt.ylabel("Car Value (₹ Lakhs)")
plt.grid(True)
plt.legend()
plt.show()
# Data Imputing Example
# Imputing means filling missing values with some strategy
# Strategies: mean, median, most_frequent, constant
# Mean Imputation : Mean meaning average of all values
# Median Imputation : Median meaning middle value of all values
# Most Frequent Imputation : Most Frequent meaning value which is most repeated
# Constant Imputation : Constant meaning value which we provide

import pandas as pd
from sklearn.impute import SimpleImputer

# Sample data
df = pd.DataFrame({
    "age": [20, 30, None, 50],
    "salary": [50000, None, 70000, 80000]
})

# mean, median, most_frequent, constant
# Mean Imputation : Mean meaning average of all values
# Median Imputation : Median meaning middle value of all values
# Most Frequent Imputation : Most Frequent meaning value which is most repeated
# Constant Imputation : Constant meaning value which we provide
# Mean
mean_df = df.copy()
imputer = SimpleImputer(strategy="mean") 
mean_df["age"] = imputer.fit_transform(mean_df[["age"]])
mean_df["salary"] = imputer.fit_transform(mean_df[["salary"]])
print("After Mean Imputation:\n", mean_df, "\n")

# Median
median_df = df.copy()
imputer = SimpleImputer(strategy="median") 
median_df["age"] = imputer.fit_transform(median_df[["age"]])
median_df["salary"] = imputer.fit_transform(median_df[["salary"]])
print("After Median Imputation:\n", median_df, "\n")

# Most Frequent
freq_df = df.copy()
imputer = SimpleImputer(strategy="most_frequent") 
freq_df["age"] = imputer.fit_transform(freq_df[["age"]])
freq_df["salary"] = imputer.fit_transform(freq_df[["salary"]])
print("After Most Frequent Imputation:\n", freq_df, "\n")

# Constant
const_df = df.copy()
imputer = SimpleImputer(strategy="constant", fill_value=0) 
const_df["age"] = imputer.fit_transform(const_df[["age"]])
const_df["salary"] = imputer.fit_transform(const_df[["salary"]])
print("After Constant Imputation:\n", const_df, "\n")import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load data
data = pd.read_csv("CarSaleData.csv")

# Features (X) and Target (y)
X = data.drop("Price", axis=1)
y = data["Price"]

# -----------------------------
# 1. Label Encoding (for ordinal-like or quick encoding)
# -----------------------------
label_encoder = LabelEncoder()
X_label_encoded = X.copy()
X_label_encoded["Brand"] = label_encoder.fit_transform(X["Brand"])
X_label_encoded["Color"] = label_encoder.fit_transform(X["Color"])

# -----------------------------
# 2. One-Hot Encoding (better for categorical data)
# -----------------------------
column_transformer = ColumnTransformer(
    transformers=[
        ("onehot", OneHotEncoder(sparse_output=False, drop="first"), ["Brand", "Color"])
    ],
    remainder="passthrough"  # keep other columns
)

X_onehot = column_transformer.fit_transform(X)
X_onehot = pd.DataFrame(X_onehot, columns=column_transformer.get_feature_names_out())
print("One-Hot Encoded Data:\n", X_onehot.head(), "\n")import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load data
data = pd.read_csv("CarSaleData.csv")
# print("Original Data:\n", data.head(), "\n")

# Features (X) and Target (y)
X = data.drop("Price", axis=1)
y = data["Price"]

# -----------------------------
# 1. Label Encoding (for ordinal-like or quick encoding)
# -----------------------------
label_encoder_brand = LabelEncoder()
label_encoder_color = LabelEncoder()
X_label_encoded = X.copy()
X_label_encoded["Brand"] = label_encoder_brand.fit_transform(X["Brand"])
X_label_encoded["Color"] = label_encoder_color.fit_transform(X["Color"])

for i, item in enumerate(label_encoder_brand.classes_):
    print(f"Label {i} is for {item}")

print("--------------------------------------- \n")
for i, item in enumerate(label_encoder_color.classes_):
    print(f"Label {i} is for {item}")

print("\nLabel Encoded Data:\n", X_label_encoded.head(), "\n")import pandas as pd
import statsmodels.api as sm
import sys
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Load dataset
if not os.path.exists("SalaryData.csv"):
	sys.exit("Error: 'SalaryData.csv' not found in the current directory.")

data = pd.read_csv("SalaryData.csv")
required_columns = {"Title", "Experience", "Salary"}
if not required_columns.issubset(data.columns):
	sys.exit(f"Error: CSV file must contain columns: {required_columns}")

y = data["Salary"]

# --- Case 1: Keeping ALL dummy variables (multicollinearity problem) ---
dummies_all = pd.get_dummies(data["Title"], drop_first=False)
X_all = pd.concat([data[["Experience"]], dummies_all], axis=1)
X_all = sm.add_constant(X_all)  # add intercept
X_all = X_all.astype(float)  # Ensure all columns are float

# --- Case 2: Dropping ONE dummy variable (fixes the issue) ---
dummies_drop = pd.get_dummies(data["Title"], drop_first=True)
X_drop = pd.concat([data[["Experience"]], dummies_drop], axis=1)
X_drop = sm.add_constant(X_drop)
X_drop = X_drop.astype(float)  # Ensure all columns are float

# Split data into train and test sets (use same split for both cases)
X_all_train, X_all_test, y_train, y_test = train_test_split(X_all, y, test_size=0.2, random_state=42)
X_drop_train, X_drop_test, _, _ = train_test_split(X_drop, y, test_size=0.2, random_state=42)

# Fit models on training data
model_all = sm.OLS(y_train, X_all_train).fit()
print("Keeping ALL dummies (multicollinearity case):\n\n")
print(model_all.summary())

model_drop = sm.OLS(y_train, X_drop_train).fit()
print("\n\nDropping ONE dummy (correct way):")
print(model_drop.summary())

print("Train R^2 Score (all dummies):", r2_score(y_train, model_all.predict(X_all_train)))
print("Test R^2 Score (all dummies):", r2_score(y_test, model_all.predict(X_all_test)))
print("Train R^2 Score (drop one dummy):", r2_score(y_train, model_drop.predict(X_drop_train)))
print("Test R^2 Score (drop one dummy):", r2_score(y_test, model_drop.predict(X_drop_test)))
# Data Scaling Example
# Scaling means transforming data to a specific range
# Two common techniques are Normalization and Standardization   
# MinMaxScaler (Normalization) : Scales data to a fixed range - usually 0 to 1
# StandardScaler (Standardization) : Scales data to have mean=0 and variance=1
## Which means it centers the data around 0 and scales it based on standard deviation
## Example : If mean=50 and std=10, then value 60 will be transformed to (60-50)/10 = 1

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# -----------------------------
# 1. Create sample dataset
# -----------------------------
df = pd.read_csv("SpendingData.csv")
X=df.drop('Spendings', axis=1)  # Features: all columns except 'Spending Score (1-100)'
y=df['Spendings']  # Target: 'Spending

lr = LinearRegression()
lr.fit(X, y)
print("Without Scaling:")
print("Coefficients:", lr.coef_)
print("Intercept:", lr.intercept_)
print("Predictions:", lr.predict(pd.DataFrame([[50,45611]],columns=['Age','Salary'])), "\n")

# -----------------------------
# 3. Normalization (Min-Max scaling: values between 0-1)
# -----------------------------
scaler_minmax = MinMaxScaler()
X_normalized = scaler_minmax.fit_transform(X)

print("Original Data:\n", pd.DataFrame(X).head(), "\n")
print("Normalized Data (0-1 range):\n", pd.DataFrame(X_normalized).head(), "\n")


# -----------------------------
# 3. Linear Regression with StandardScaler
# -----------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

lr_scaled = LinearRegression()
lr_scaled.fit(X_scaled, y)

print("With Scaling:")
print("Coefficients:", lr_scaled.coef_)
print("Intercept:", lr_scaled.intercept_)
print("Predictions:", lr_scaled.predict(pd.DataFrame([[50,45611]])), "\n")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures

# Load the dataset safely
try:
    df = pd.read_csv('data.csv')
except FileNotFoundError:
    print("Error: data.csv not found!")
    exit()

# # Print dataset details
# print("############# Columns : ")
# print(df.columns)

# print("############# Info : ")
# print(df.info())  # No need for print()

# print("############# Describe : ")
# print(df.describe())  # No need for print()

# Ensure required columns exist
if 'a' not in df.columns or 'b' not in df.columns:
    print("Error: Columns 'a' or 'b' not found in dataset")
    exit()

# Plot scatter graph or "Visualize the data"
print("############# Plot Graph : ")
plt.scatter(df['a'], df['b'])
plt.xlabel('a - Number')
plt.ylabel('b - Square of a')
plt.title('Number vs Square of Number')
#plt.show()  # Uncomment to see graph


corr = df['a'].corr(df['b'])
print("Correlation between a and b:", corr)

print("\n------------------------------------\n\n")
# Create and train the model
############# Create object of model"
model = LinearRegression()

X = df[['a']]  # Ensure X is a 2D array
y = df['b']

# Why we need PolynomialFeatures?
# Because our data is not linear, it's quadratic (b = a^2)
# We need to transform our features to include polynomial terms
# This will help the linear regression model to fit a non-linear relationship
# For example, if a = 3, we want to include a^2 = 9 as a feature
# So that the model can learn the relationship between a and b
# Without this transformation, the model will try to fit a straight line
# to the data, which will not work well for quadratic data
poly = PolynomialFeatures(degree=2,include_bias=False)  # Since b = a², we use degree=2
a_poly = poly.fit_transform(X)
# print("##### Transformed features : ")
# print(a_poly)

# print("##### Train the model")
model.fit(a_poly, df['b'])

# # Predict the square of 12
# print("##### Predict the square")
a_test = poly.transform(pd.DataFrame([[12]],columns=['a']))
square_predict = model.predict(a_test)
print("Square of 12 is:", square_predict[0])

import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

# Load the model from pickle file
# rb = read binary
with open('model.pkl', 'rb') as file:
    model = pickle.load(file)

salaries = model.predict(pd.DataFrame([[15]], columns=['Experience']))

# Print the predicted salary
print("Salary of 15 years of experience is:", salaries)
# numpy helps to do mathematical calculations for scientific, numeric, data-intensive
import numpy as np
# pandas helps to handle data in tabular form (rows and columns)
import pandas as pd
import matplotlib.pyplot as plt



# Load the dataset from a CSV file
# Data Frame is a 2D labeled data structure with columns of potentially different types
# It is generally the most commonly used pandas object
# Like an SQL table or Excel spreadsheet
df = pd.read_csv("salary_data.csv")

# Print dataset details
#print("############# Columns : ")
print(df.columns)

#print("############# Info : ")
df.info()  # No need for print()

#print("############# Describe : ")
print(df.describe())

# Mean, Median, Mode
# Mean is the average of all values
# Median is the middle value when all values are sorted
# Mode is the most frequently occurring value
# Mean, Median, Mode are used to understand the distribution of data
print("Mean Salary: ", df['Salary'].mean())
print("Median Salary: ", df['Salary'].median())
print("Mode Salary: ", df['Salary'].mode()[0])

# Plot scatter graph
# Visualize the data
# A scatter plot is a graph that shows the relationship between two variables
# The x-axis represents one variable, and the y-axis represents the other
# Each point on the graph represents a single observation
# Scatter plots are used to see if there is a relationship between two variables
# Scatter plots are used to see if there is a correlation between two variables
# Scatter plots are used to see if there is a pattern between two variables
# Scatter plots are used to see if there is a trend between two variables
plt.scatter(df['Experience'], df['Salary'])
plt.xlabel('Experience')
plt.ylabel('Salary')
plt.title('Experience vs Salary')
plt.show()  # Uncomment to see graph


# Find correlation (Relation is strong or weak between two variables)
# Correlation is a normalized form of covariance
# It is a measure of how much two variables change together
# Correlation ranges from -1 to 1
# 1 means that the variables are perfectly correlated
# 0 means that the variables are not correlated
# -1 means that the variables are perfectly inversely correlated
correlation = df['Experience'].corr(df['Salary'])
print("Correlation: ", correlation)


# Find covariance (Find is there any relation between two variables)
# Covariance is a measure of how much two variables change together
# Positive covariance means that the variables are directly proportional
# Negative covariance means that the variables are inversely proportional
# Zero covariance means that the variables are not related
# Covariance can be any value
# Covariance is not normalized,standardized,bounded,percentage,probability,correlation
## Covariance of x and y = Σ((x - mean(x)) * (y - mean(y))) / (n-1)
## Covariance of x = Covariance of y
##[[cov(x,y)]] = cov(x,x) = cov(y,y)
covariance = np.cov(df['Experience'], df['Salary'])
print("Covariance: ", covariance)

import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle

# Load the dataset from a CSV file
df = pd.read_csv("salary_data.csv")

# Separate the features (independent variables) and the target (dependent variable)
x = df.drop('Salary', axis=1)  # Features: all columns except 'Salary'
y = df['Salary']  # Target: 'Salary' column

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model to the data
model.fit(x, y)

# Predict the salary for 15 years of experience
#salaries = model.predict([[15]])

# Model training remembers feature names.
# During prediction, always pass data in the same shape and with the same column names.
# If names don’t match, sklearn warns you to prevent wrong predictions.
salaries = model.predict(pd.DataFrame([[15]], columns=['Experience']))

# Print the predicted salary
print("Salary of 15 years of experience is:", salaries)

with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression


# Load the dataset from a CSV file
# Data Frame is a 2D labeled data structure with columns of potentially different types
# It is generally the most commonly used pandas object
# Like an SQL table or Excel spreadsheet
df = pd.read_csv("salary_data.csv")

# Print dataset details
#print("############# Columns : ")
print(df.columns)

#print("############# Info : ")
df.info()  # No need for print()

#print("############# Describe : ")
print(df.describe())

# Mean, Median, Mode
# Mean is the average of all values
# Median is the middle value when all values are sorted
# Mode is the most frequently occurring value
# Mean, Median, Mode are used to understand the distribution of data
print("Mean Salary: ", df['Salary'].mean())
print("Median Salary: ", df['Salary'].median())
print("Mode Salary: ", df['Salary'].mode()[0])

# Plot scatter graph
# Visualize the data
# A scatter plot is a graph that shows the relationship between two variables
# The x-axis represents one variable, and the y-axis represents the other
# Each point on the graph represents a single observation
# Scatter plots are used to see if there is a relationship between two variables
# Scatter plots are used to see if there is a correlation between two variables
# Scatter plots are used to see if there is a pattern between two variables
# Scatter plots are used to see if there is a trend between two variables
plt.scatter(df['Experience'], df['Salary'])
plt.xlabel('Experience')
plt.ylabel('Salary')
plt.title('Experience vs Salary')
plt.show()  # Uncomment to see graph

# Find covariance (Find is there any relation between two variables)
# Covariance is a measure of how much two variables change together
# Positive covariance means that the variables are directly proportional
# Negative covariance means that the variables are inversely proportional
# Zero covariance means that the variables are not related
# Covariance can be any value
# Covariance is not normalized,standardized,bounded,percentage,probability,correlation
## Covariance of x and y = Σ((x - mean(x)) * (y - mean(y))) / (n-1)
## Covariance of x = Covariance of y
##[[cov(x,y)]] = cov(x,x) = cov(y,y)
covariance = np.cov(df['Experience'], df['Salary'])
print("Covariance: ", covariance)

# Find correlation (Relation is stron ok)
# Correlation is a normalized form of covariance
# It is a measure of how much two variables change together
# Correlation ranges from -1 to 1
# 1 means that the variables are perfectly correlated
# 0 means that the variables are not correlated
# -1 means that the variables are perfectly inversely correlated
correlation = df['Experience'].corr(df['Salary'])
print("Correlation: ", correlation)

# Separate the features (independent variables) and the target (dependent variable)
# Why drop function ? : We are using drop since independent variable should be two dimention array
# Why x is 2D array ? : x is 2D array since it is independent variable
# Why axis=1 ? : axis=1 means we are dropping column
# Why axis=0 ? : axis=0 means we are dropping row
# Why x and y ? : x is independent variable and y is dependent variable as per convention
x = df.drop('Salary', axis=1) # We are usiong drop since independent variable should be two dimention array
y=df['Salary']

# Initialize the Linear Regression model
# Why Linear Regression ? : Linear regression is used for regression problems
# Regression is used to predict a continuous value
# Linear regression is used to predict a continuous value using one or more independent variables
# Linear regression is used to predict a continuous value using a linear equation
# Linear regression is used to predict a continuous value using a straight line
# Planning to use equation y = mx + c where m is slope/coeficient and c is intercept
model = LinearRegression()

# Fit the model to the data
# Why x and y ? : x is independent variable and y is dependent variable as per convention
# model.fit is used to train the model
# .fit() takes your data → checks it → applies the learning algorithm → finds optimal parameters 
# → saves them inside the model → returns the trained model.
model.fit(x,y)

# Predict the salary for 15 years of experience
# Why model.predict ? : model.predict is used to predict the value
# Why pd.DataFrame ? : pd.DataFrame is used to create a data frame
# Why predict need DataFrame ? : predict need DataFrame to predict the value and it should be 2D array
# DataFrame is a 2D labeled data structure with columns of potentially different types
# It is generally the most commonly used pandas object
# Like an SQL table or Excel spreadsheet
salaries = model.predict(pd.DataFrame([[15]],columns=['Experience']))
print("Salary of 15 years of experience is:", salaries[0])

# Since we have model which is trained, we can plot the BEST FIT REGRESSION LINE

plt.scatter(df['Experience'], df['Salary'],label='Observed Data')
plt.scatter(df['Experience'], model.predict(x), color='blue',label='Predicted Data')
plt.plot(df['Experience'], model.predict(x), color='red',label='Best Fit Line')
plt.legend()
plt.show()  # Uncomment to see graph


# Understand how model is bullt (internal formula)
# y = mx + c
# y = dependent variable
# x = independent variable
# m = coefficient
# c = intercept
# y = mx + c
# y = model.coef_ * x + model.intercept_
# y = model.coef_ * 15 + model.intercept_
# y = model.coef_[0] * 15 + model.intercept_ // Since model.coef_ is 1D array
yearExperience = 15
salary = model.coef_[0] * yearExperience + model.intercept_
print("Salary of 15 years of experience is:", salary)import pandas as pd
from sklearn.linear_model import LinearRegression
import pickle # importing pickle module to save the model

# Load the dataset from a CSV file
df = pd.read_csv("salary_data.csv")

# Separate the features (independent variables) and the target (dependent variable)
x = df.drop('Salary', axis=1)  # Features: all columns except 'Salary'
y = df['Salary']  # Target: 'Salary' column

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model to the data
model.fit(x, y)

# Predict the salary for 15 years of experience
#salaries = model.predict([[15]])

# Model training remembers feature names.
# During prediction, always pass data in the same shape and with the same column names.
# If names don’t match, sklearn warns you to prevent wrong predictions.
salaries = model.predict(pd.DataFrame([[15]], columns=['Experience']))

# Print the predicted salary
print("Salary of 15 years of experience is:", salaries)

with open('model.pkl', 'wb') as file:
    pickle.dump(model, file) # Saving the model to a file named 'model.pkl' in write binary mode
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression

# Load the dataset from a CSV file
df = pd.read_csv("salary_data.csv")

# Separate the features (independent variables) and the target (dependent variable)
x = df.drop('Salary', axis=1) 
y=df['Salary']

# Initialize the Linear Regression model
model = LinearRegression()

# Fit the model to the data
model.fit(x,y)

# Since we have model which is trained, we can plot the BEST FIT REGRESSION LINE
plt.scatter(df['Experience'], df['Salary'],label='Observed Data')
plt.scatter(df['Experience'], model.predict(x), color='blue',label='Predicted Data')
plt.plot(df['Experience'], model.predict(x), color='red',label='Best Fit Line')
plt.legend()
plt.show()  

# Understand how model is bullt (internal formula)
# y = mx + c
# y = dependent variable
# x = independent variable
# m = coefficient
# c = intercept
# y = mx + c
# y = model.coef_ * x + model.intercept_
# y = model.coef_ * 15 + model.intercept_
# y = model.coef_[0] * 15 + model.intercept_ # Since model.coef_ is 1D array
# yearExperience = 15
# salary = model.coef_[0] * yearExperience + model.intercept_
# print("Salary of 15 years of experience is:", salary)import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load the dataset from a CSV file
df = pd.read_csv("Advertising.csv")
#print(df.head())

# Exploratoring Data Analysis (EDA )
#print("############# Columns : ")
#print(df.columns)

# Read first few records
#print("############# Head : ")
#print(df.head())

# Read last few records
#print("############# Tail : ")
#print(df.tail())

# Get some basic information about the dataset
#print("############# Info : ")
#df.info()  # No need for print()

# Get statistical information about the dataset
#print("############# Describe : ")
#print(df.describe())

# Visualize the data
# A scatter plot is a graph that shows the relationship between two variables
# The x-axis represents one variable, and the y-axis represents the other
# Each point on the graph represents a single observation

## Relationship between TV and Sales seems not good relation
plt.scatter(df['TV'], df['sales'])
plt.xlabel('TV')
plt.ylabel('sales')
plt.title('TV vs Sales')
plt.show()  # Uncomment to see graph

## Relationship between TV and Radio seems not good relation
plt.scatter(df['radio'], df['sales'])
plt.xlabel('TV')
plt.ylabel('radio')
plt.title('Radio vs Sales')
plt.show()  # Uncomment to see graph

## Relationship between TV and News paper seems not good relation
plt.scatter(df['newspaper'], df['sales'])
plt.xlabel('newspaper')
plt.ylabel('newspaper')
plt.title('newspaper vs Sales')
plt.show()  # Uncomment to see graph

# Find the corelation coefficient
correlation = df.corr()
#print("Correlation between TV and Sales: ")
#print(correlation)

# Create independent variables
x = df.drop(['sales'], axis=1)  # Features: all columns except 'sales'
# x = x.drop('newspaper', axis=1)  #This can be also done
y = df['sales']  # Target: 'sales' column

# Build the model
model = LinearRegression()
model.fit(x, y) # x is independent variable and y is dependent variable   

# Test the model
sales = model.predict(pd.DataFrame([[100, 100,100]], columns=['TV', 'radio','newspaper']))
print("Sales for 100 TV, 100 Radio, 100 Newspaper is:", sales[0])
sales = model.predict(pd.DataFrame([[147, 23,19]], columns=['TV', 'radio','newspaper']))
print("Sales for 147 TV, 23 Radio, 19 Newspaper is:", sales[0])

## Some time some independent data might not found much relevant
## We should drop that data to get better prediction and speed
x = df.drop(['sales','newspaper'], axis=1)
y = df['sales']
model.fit(x, y)
sales = model.predict(pd.DataFrame([[100, 100]], columns=['TV', 'radio']))
print("Sales for 100 TV, 100 Radio is:", sales[0])
sales = model.predict(pd.DataFrame([[147, 23]], columns=['TV', 'radio']))
print("Sales for 147 TV, 23 Radio is:", sales[0])# Author: Vaibhav Zodge
# Outcome: Model Evaluation
# We should evaluate our model for the error rates
# So what we can do is we can split our data into two parts, training and testng
# And then on the basis of Testing Data we try to predict the sales and see the error rate
# We can use different metrics to evaluate our model or errors
# Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R2 Score (Coefficient of Determination)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import root_mean_squared_error
from sklearn.metrics import r2_score

# Load the dataset from a CSV file
df = pd.read_csv("Advertising.csv")

x = df.drop(['sales'], axis=1)  # Features: all columns except 'sales'
y = df['sales']  # Target: 'sales' column

# Split the data into training and testing sets
# 80% of the data will be used for training and 20% for testing
# random_state is used to ensure that the data is split in the same way every time
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)


# Build the model
model = LinearRegression()
model.fit(x_train, y_train) # Train the model using the training sets


y_pred = model.predict(x_test)

# Mean Squared Error (MSE)
# Mean Squared Error (MSE) is the average of the squared differences between the predicted and actual values.
# ✅ Use MSE if you want to penalize big errors more (useful when large mistakes are costly).
# It is a measure of how close the regression line is to the actual data points.
# The smaller the MSE, the closer the fit is to the data.
# The MSE has the units squared of whatever is plotted on the vertical axis.
# value of mse is 0 means model is good
# value of mse is near to 0 means model is good
# value of mse is near to 1 means model is good
# value of mse is near to 0 means model is bad
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Mean Absolute Error (MAE)
# Mean Absolute Error (MAE) is the average of the absolute differences between the predicted and actual values.
## ✅ Use MAE if you want a simple interpretation (average error in actual units).
# It is a measure of how close the regression line is to the actual data points.
# The smaller the MAE, the closer the fit is to the data.
# The MAE has the same units as the data being plotted on the vertical axis.
# value of mae is 0 means model is good
# value of mae is near to 0 means model is good
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)


# Root Mean Squared Error (RMSE)
# Root Mean Squared Error (RMSE) is the square root of the average of the squared differences between the predicted and actual values.
# It is a measure of how close the regression line is to the actual data points.
# The smaller the RMSE, the closer the fit is to the data.
# The RMSE has the same units as the data being plotted on the vertical axis.
# value of rmse is 0 means model is good
# Lower the error, better the model
rmse = root_mean_squared_error(y_test, y_pred)
print("Root Mean Squared Error:", rmse)

# R2 Score (Coefficient of Determination)
# R-squared is a statistical measure of how close the data are to the fitted regression line.
# It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.
# The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model.
# value of r2 score is between 0 to 1
# value of r2 score is near to 1 means model is good
# value of r2 score is near to 0 means model is bad
# value of r2 score is negative means model is worst
r2 = r2_score(y_test, y_pred)
print("R2 Score:", r2)

# Test the model
sales = model.predict(pd.DataFrame([[100, 100,100]], columns=['TV', 'radio','newspaper']))
print("Sales for 100 TV, 100 Radio, 100 Newspaper is:", sales[0])
# Author: Vaibhav Zodge
# Outcome: Model Testing
# We should test our model for the sales prediction
# To test the model, we should split our data into two parts, training and testing
# And then on the basis of Testing Data we try to predict the sales
# and see the error rate

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Load the dataset from a CSV file
df = pd.read_csv("Advertising.csv")

x = df.drop(['sales'], axis=1)  # Features: all columns except 'sales'
y = df['sales']  # Target: 'sales' column


# Split the data into training and testing sets
# 80% of the data will be used for training and 20% for testing
# random_state is used to ensure that the data is split in the same way every time
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=42)


# Build the model
model = LinearRegression()
model.fit(x_train, y_train) # Train the model using the training sets

# Test the model
sales = model.predict(pd.DataFrame([[100, 100,100]], columns=['TV', 'radio','newspaper']))
print("Sales for 100 TV, 100 Radio, 100 Newspaper is:", sales[0])
from flask import Flask, request, jsonify
import pickle

app = Flask(__name__)

classes = ['setosa', 'versicolor', 'virginica']

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    features = data.get('features')

    if features is None:
        return jsonify({'error': 'Missing features in request'}), 400
    
    try:
        # Load the model
        with open('model.pkl', 'rb') as file:
            model = pickle.load(file)

        # Make prediction
        prediction = model.predict([features])

        return jsonify({'prediction': classes[int(prediction[0])]})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host="0.0.0.0", port=8083, debug=True)
# Author: Vaibhav Zodge
# Outcome: Learn Data Cleansing 
#          Model Evaluation using Confusion Matrix, Precision, F1 Score

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, classification_report
import pickle

################### Data Load and Primary Analysis##################

df = pd.read_csv("iris.csv") 


############################ Data Cleansing ########################


encoder = LabelEncoder()
encoder.fit(df['species']) 
print(encoder.classes_ ) # Print all the classed that encoder found

# Print encoder class and respective encoded number. Might needed for mapping and putting value back
for class_label, encoded_label in enumerate(encoder.classes_):
    print(f"Class: {encoded_label}, Encoded as: {class_label}")


df['species'] = encoder.transform(df['species'])


############################ Data Visulization/Analysis ########################


print(df.corr())



############################ Split the data ########################
x = df.drop('species', axis=1)  # Features: all columns except 'species'
y = df['species']  # Target: 'species' column


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2131546)

############################ Create Model ########################
model = LogisticRegression()
model.fit(x_train, y_train) # Train the model using the training sets


############################ Save the model ########################

# Save the model into a pkl file

with open('model.pkl', 'wb') as file:
    pickle.dump(model, file)